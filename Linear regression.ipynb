{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm\n",
    "\n",
    "In this notebook we implement from scratch the simple learning algorithm linear regression. The idea is easy. We have some data X of size (m,n) where each row is a data point characterized by n features. We also have the targets associated with each example in the form of a (m,1) vector. We want to model this data with a hypothesis of the form\n",
    "\n",
    "$$ h(X) = \\beta_0 + X\\beta $$\n",
    "\n",
    "It is simpler to add an extra feature for the parameter $\\beta_0$ by including a column of ones into X. The hypothesis then looks like\n",
    "\n",
    "$$ h(X) = X\\beta $$\n",
    "\n",
    "The goal is to try to minimize the difference between the predictions and the actual data, which is computed using the cost function\n",
    "\n",
    "$$ J(\\beta) = (Y-X\\beta)^T(Y-X\\beta) $$\n",
    "\n",
    "There are two ways of finding the best set of parameters $\\beta$ to describe the data:\n",
    "\n",
    "- Gradient descent: Initialize the weights to 0 and iteratively move it along the gradient of the cost function. The algorithm consists of repeating the following step many times\n",
    "\n",
    "$$ \\beta = \\beta - \\alpha\\frac{\\partial J}{\\partial\\beta} $$\n",
    "\n",
    "The parameter $\\alpha$ is the learning rate and the gradient is given by\n",
    "\n",
    "$$ \\frac{\\partial J}{\\partial\\beta} = -2X^T(Y-X\\beta) $$\n",
    "\n",
    "We iterate like this until the cost stops decreasing.\n",
    "\n",
    "- There is actually an exact solution to this optimization problem, called the normal equation. The optimal weights can be found to be\n",
    "\n",
    "$$ \\beta = (X^T X)^{-1}X^T Y $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization\n",
    "\n",
    "To prevent overfitting it is useful to introduce regularization, which simply amounts to adding a term to the minimization problem. The two most common schemes used are:\n",
    "\n",
    "- L1: Minimize the cost function $J(\\beta) + \\lambda\\sum_{i=1}^n|\\beta_i|$. It's important to not regularize the bias $\\beta_0$. This is implemented by adding a term to the gradient descent of all the weights except $\\beta_0$\n",
    "\n",
    "$$ \\beta_i = \\beta_i - \\alpha \\frac{\\partial J}{\\partial\\beta_i} - \\lambda\\,sign(\\beta_i) $$\n",
    "\n",
    "- L2: Minimize the cost function $J(\\beta) + \\lambda\\sum_{i=1}^n\\beta_i^2$. The new gradient descent algorithm is\n",
    "\n",
    "$$ \\beta_i = \\beta_i - \\alpha \\frac{\\partial J}{\\partial\\beta_i} - \\lambda\\beta_i $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error metrics\n",
    "\n",
    "To assess the success of the prediction two metrics are the most common to use. They are both based on the cost function, which is also called residual sum of squares (RSS):\n",
    "    \n",
    "- Residual standard error (RSE): This is basically the standard deviation between the prediction and the data and it is calculated from\n",
    "\n",
    "$$ RSE = \\sqrt{\\frac{RSS}{m-2}} $$\n",
    "\n",
    "It is an absolute estimation of the error and we want it to be as small as possible.\n",
    "\n",
    "- $R^2$: This is an index that indicates the proportion of the variance in the data that is captured by the model. This is based off the actual variance of the data, called True sum of squared (TSS)\n",
    "\n",
    "$$ TSS = (Y - \\bar{Y})^T(Y - \\bar{Y}) $$\n",
    "\n",
    "The coefficient is computed by\n",
    "\n",
    "$$ R^2 = 1-\\frac{RSS}{TSS} $$\n",
    "\n",
    "and it takes values between 0 and 1. The higher it is the better the model is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering\n",
    "\n",
    "There are two steps that may be useful to do before training the model:\n",
    "\n",
    "- Normalization: It is important that all the features are on the same scale. This can be achieved by taking the training data and transforming each feature according to\n",
    "\n",
    "$$ X_i \\rightarrow \\frac{X_i - \\mu_i}{\\sigma_i} $$\n",
    "\n",
    "with $\\mu_i$ the mean of the feature and $\\sigma_i$ the standard deviation. This should put all the features mostly on a scale from -1 to 1. Note that the same conversion, using the training data, must be used when evaluating the model on new data.\n",
    "\n",
    "- Polynomial: We can use the features and combine them into all the possible multiples of each other up to a certain degree. This will generate a model that has more predicting power since it doesn't just assume that the data is linear but it uses a more general hypothesis. This is called polynomial regression. It however increases quickly the number of features so it may lead to overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import inv\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "class lin_reg:\n",
    "    '''Model that does linear regression on the data. There is also the option of transforming the data\n",
    "    to do polynomial regression.'''\n",
    "    \n",
    "    def __init__(self, degree = None, normalize = False):\n",
    "        '''Set parameters to decide if we normalize the data and if we create polynomial features.'''\n",
    "        if degree >=2:\n",
    "            self.degree = degree\n",
    "        else:\n",
    "            self.degree = None\n",
    "        if normalize:\n",
    "            self.normalize = True\n",
    "    \n",
    "    def train(self, X, Y, alpha = 0.0001, limit = 100000, normal = False, lambd = 0, regularization = None):\n",
    "        '''Train the model on a dataset from the examples X and the labels Y.\n",
    "        The algorithm uses gradient descent with learning rate 'alpha' for a max of 'limit' steps.\n",
    "        X must in the format (# examples)x(# features) and Y is a column array.\n",
    "        The final weights as well as a list of the costs calculated through the training \n",
    "        can be extracted using methods .weights and .cost.\n",
    "        Option to use the exact solution using \"normal\" instead of gradient descent.\n",
    "        There are the option of including L1 or L2 regularization.'''\n",
    "        \n",
    "        # Normalize the data and save the normalization parameters\n",
    "        if self.normalize:\n",
    "            self.mean = X.mean(axis=0)\n",
    "            self.std = X.std(axis=0)\n",
    "            X = (X - self.mean) / self.std\n",
    "        # Transform data to polynomial features if asked to\n",
    "        if self.degree != None:\n",
    "            X = self.polynomial(X, self.degree)\n",
    "        # Extract number of training examples and features from X\n",
    "        (m, n) = X.shape\n",
    "        # Add a column of ones to X for the bias\n",
    "        X = np.append(np.ones((m,1)), X, axis = 1)\n",
    "        Y = np.reshape(Y, (m,1))\n",
    "            \n",
    "        if not normal:\n",
    "            # Initialize the weights to zero\n",
    "            self.weights = np.zeros((n+1 ,1))\n",
    "            # Initialize the cost\n",
    "            i = 0\n",
    "            self.cost = []\n",
    "            self.cost.append(np.matmul(np.transpose(Y - np.matmul(X, self.weights)), Y - np.matmul(X, self.weights)))\n",
    "            # Update weights with gradient and compute new cost\n",
    "            while True:\n",
    "                self.weights = self.weights - alpha * self.gradient(X, Y, self.weights, lambd, regularization)\n",
    "                self.cost.append(np.matmul(np.transpose(Y - np.matmul(X, self.weights)), Y - np.matmul(X, self.weights)))\n",
    "                # Stop upgrading if cost doesn't lower of if reached limit\n",
    "                if self.cost[i + 1] < self.cost[i]:\n",
    "                    if i % 10000 == 0:\n",
    "                        print(i, 'steps done', end=\"\\r\")\n",
    "                    i += 1\n",
    "                else:\n",
    "                    break\n",
    "                if i > limit:\n",
    "                    print('Reached the limit')\n",
    "                    break\n",
    "        else:\n",
    "            self.weights = np.matmul(np.matmul(inv(np.matmul(np.transpose(X), X)), np.transpose(X)), Y)\n",
    "\n",
    "            \n",
    "    def gradient(self, X, Y, weights, lambd, regularization):\n",
    "        '''Compute the gradient involved in gradient descent, for different regularization schemes.'''\n",
    "        \n",
    "        (m, n) = X.shape\n",
    "        basic_grad = -2 * np.matmul(np.transpose(X), Y - np.matmul(X, weights))\n",
    "        if regularization == 'L1':\n",
    "            return basic_grad + lambd * np.insert(np.sign(weights[1:]), 0, 0).reshape(n, 1) \n",
    "        elif regularization == 'L2':\n",
    "            return basic_grad + lambd * np.insert(weights[1:], 0, 0).reshape(n, 1)\n",
    "        else:\n",
    "            return basic_grad\n",
    "    \n",
    "    \n",
    "    def predict(self, X):\n",
    "        '''Use data X and trained model to predict labels using linear regression hypothesis.\n",
    "        Output is an array with one value for each data point.'''\n",
    "    \n",
    "        # Normalize the data if asked to\n",
    "        if self.normalize:\n",
    "            X = (X - self.mean) / self.std\n",
    "        # Transform data to polynomial features if asked to\n",
    "        if self.degree != None:\n",
    "            X = self.polynomial(X, self.degree)\n",
    "        # Extract number of training examples and features from X\n",
    "        (m, n) = X.shape\n",
    "        # Add a column of ones to X for the bias\n",
    "        X = np.append(np.ones((m,1)), X, axis = 1)\n",
    "        return np.matmul(X, self.weights)\n",
    "    \n",
    "    \n",
    "    def error(self, Y, Y_pred, metric):\n",
    "        '''Computes the error of Y_pred compared to true answer Y.\n",
    "        Option of using the R2 or RSE as metrics.'''\n",
    "        \n",
    "        # Reshape data and use it to make a prediction\n",
    "        m = Y.shape[0]\n",
    "        Y = np.reshape(Y, (m,1))\n",
    "        # Compute total squared error for use later\n",
    "        rss = np.matmul(np.transpose(Y - Y_pred), Y - Y_pred)\n",
    "        if metric == 'rse':\n",
    "            rse_error = np.sqrt(rss/(m-2))\n",
    "            print('RSE error is: ', rse_error)\n",
    "        elif metric == 'r2':\n",
    "            tss = np.matmul(np.transpose(Y - np.mean(Y)), Y - np.mean(Y))\n",
    "            r2_error = 1 - rss/tss\n",
    "            print('R squared error is: ', r2_error)\n",
    "        else:\n",
    "            print('Wrong metric specification!')\n",
    "        \n",
    "    def polynomial(self, X, degree):\n",
    "        '''Combines the features into all possible combinations to form a polynomial of a given degree.<\n",
    "        This should be used before doing anything else and on all the data.'''\n",
    "        \n",
    "        (m,n) = X.shape\n",
    "        for deg in range(2, degree+1):\n",
    "            for combin in itertools.combinations_with_replacement(range(n), deg):\n",
    "                X = np.append(X, np.prod(X[:, list(combin)], axis=1).reshape(m,1), axis=1)\n",
    "        return X\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Future: add regularization, polynomial regression, visualization cost, normalize data\n",
    "\n",
    "Notes: normalize before poly, can't use normal equation for poly, need very small alpha (not sure if ok)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
